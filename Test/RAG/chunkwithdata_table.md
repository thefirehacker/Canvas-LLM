
# What TimeCapsule Found : pattern /Tyler|hours|paper|devices|work|ed|eab|k/gi
<START_TABLE> TABLE_ROW>3 | Longer Sequence Length | 2.55 hours | 1.88B | 205k | 2025/03/03 | d982ed5 | here</TABLE_ROW> <TABLE_ROW>2.4 | Logit Soft-capping at 30 | 4.01 hours | 3.15B | 218k | 2025/02/23 | 12eab44 | here</TABLE_ROW> <TABLE_ROW>2.3 | Dataloading tweaks | 4.26 hours | 3.31B | 216k | 2025/02/18 | d59944d | here</TABLE_ROW> <TABLE_ROW>2.2 | Muon optimizer | 4.53 hours | 3.04B | 187k | 2025/01/23 | b91c2c0 | here</TABLE_ROW> <TABLE_ROW>2.1 | Architectural changes | 7.51 hours | 5.07B | 188k | 2025/01/18 | b7bb93f | here</TABLE_ROW> <TABLE_ROW>1 | Initial baseline | 8.13 hours | 6.44B | 221k | 2025/01/16 | b3c32f8 | here</TABLE_ROW> <TABLE_ROW># | Description | Record time | Training Tokens | Tokens/Second | Date | Commit | Log</TABLE_ROW> <END_TABLE> <START_SECTION:Progress so far> Progress so far <END_SECTION> <START_PARAGRAPH> this Git Hub repo . <END_PARAGRAPH> <START_PARAGRAPH> Iâ€™ll be documenting my progress here and updating this post as I go. Code can be found in <END_PARAGRAPH> <START_PARAGRAPH> transfer my methods to an 8x H100 node for comparison with the main leaderboard. <END_PARAGRAPH> <START_PARAGRAPH> by following the same rules as the Nano GPT speedrun. If I see some success, I may try to <END_PARAGRAPH> <TABLE_ROW>I have access to | 2x RTX 4090 GPUs | and I want to see how fast I can train GPT-2 on them</TABLE_ROW> <START_PARAGRAPH> At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). <END_PARAGRAPH> <START_PARAGRAPH> Fine Web as fast as possible on an 8x H100 node. Keller Jordan maintains a
