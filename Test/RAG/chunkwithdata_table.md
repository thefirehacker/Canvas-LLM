Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here. At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). I have access to 2x RTX 4090 GPUs and I want to see how fast I can train GPT-2 on them by following the same rules as the Nano GPT speedrun. If I see some success, I may try to transfer my methods to an 8x H100 node for comparison with the main leaderboard. I’ll be documenting my progress here and updating this post as I go. Code can be found in this Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repo. Progress so far #Description Record time Training Tokens Tokens/Second Date Commit Log 1111111111111Initial baseline8.13 hours6.44B221k2025/01/16b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8hereherehereherehereherehereherehereherehereherehere 2.12.12.12.12.12.12.12.12.12.12.12.12.1Architectural changes7.51 hours5.07B188k2025/01/18b7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fhereherehereherehereherehereherehereherehereherehere 2.22.22.22.22.22.22.22.22.22.22.22.22.2Muon optimizer4.53 hours3.04B187k2025/01/23b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0hereherehereherehereherehereherehereherehereherehere 2.32.32.32.32.32.32.32.32.32.32.32.32.3Dataloading tweaks4.26 hours3.31B216k2025/02/18d59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dhereherehereherehereherehereherehereherehereherehere 2.42.42.42.42.42.42.42.42.42.42.42.42.4Logit Soft-capping at 304.01 hours3.15B218k2025/02/2312eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab44hereherehereherehereherehereherehereherehereherehere 3333333333333Longer Sequence Length2.55 hours1.88B205k2025/03/03d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5hereherehereherehereherehereherehereherehereherehere ----------------Page (0) Break---------------- 1. Initial setup and baseline Part of the goal of this project is for me to learn as I go, so I am going to start at the



# New Structure Aware
3 3 3 3 3 3 3 3 3 3 3 3 3 Longer Sequence Length 2.55 hours 1.88B 205k 2025/03/03 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 here here here here here here here here here here here here here 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 2.4 Logit Soft-capping at 30 4.01 hours 3.15B 218k 2025/02/23 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 here here here here here here here here here here here here here 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 2.3 Dataloading tweaks 4.26 hours 3.31B 216k 2025/02/18 d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d here here here here here here here here here here here here here 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 2.2 Muon optimizer 4.53 hours 3.04B 187k 2025/01/23 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 b91c2c0 here here here here here here here here here here here here here 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 2.1 Architectural changes 7.51 hours 5.07B 188k 2025/01/18 b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f here here here here here here here here here here here here here 1 1 1 1 1 1 1 1 1 1 1 1 1 Initial baseline 8.13 hours 6.44B 221k 2025/01/16 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 here here here here here here here here here here here here here # | Description | Record time | Training Tokens | Tokens/Second | Date | Commit | Log Progress so far this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo this Git Hub repo . I’ll be documenting my progress here and updating this post as I go. Code can be found in transfer my methods to an 8x H100 node for comparison with the main leaderboard. by following the same rules as the Nano GPT speedrun. If I see some success, I may try to I have access to | 2x RTX 4090 GPUs | and I want to see how fast I can train GPT-2 on them At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). Fine Web as fast as possible on an 8x H100 node. Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here . Technically, the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun the Nano GPT speedrun is to train a neural network to 3.28 validation loss on could train GPT-2 on my own hardware. Fern Fern Fern Fern Fern Fern Fern Fern Fern Fern Fern Fern Fern , Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky Braden Koszarsky , and others. I got a little inspired and wanted to see how fast I I’ve seen some some some some some some some some some some some some some really really really really really really really really really really really really really awesome awesome awesome awesome awesome awesome awesome awesome awesome awesome awesome awesome awesome GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 GPT-2 speedrun results from people like Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan Keller Jordan , March 8, 2025 How fast can I train GPT-2 on two RTX 4090 GPUs? Nano GPT Speedrun Living Worklog T Y L E R R O M E R O P O S T S ----------------Page (1) Break---------------- begin by implementing some of the notable improvements from the 8x H100 leaderboard. Waiting 8 hours for a result is too slow for effective experimentation, so I’m going to leaderboard 2. Implementing major improvements from the 8x H100 The baseline run time on my 2x RTX 4090 setup is | 8.13 hours | . Commit with the initial setup is here: b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 b3c32f8 . may need to remove this one day as it slightly increases step time. Additionally, I added wandb logging for easy tracking of training progress - optimistically I leaderboard). 6. Using Pytorch 2.5.1 (the switch from 2.4 to 2.5 gave ~9% speedup on the 8x H100 better tensor core utilization). 5. Padded the vocab size from 50257 to 50304 to make it a multiple of 128 (for 4. Removed all affine scale/bias parameters and switched to RMSNorm. 3. Improved learning rate schedule (linear warmup then linear decay). accum steps ). 262144 - that is bs of 32/device * 2 devices * 1024 sequence length * 4 gradient 2. Increased learning rate to 0.0015 and halved the batch size (total batch size is training experience of a 8x H100 machine. 1. Implemented gradient accumulation so that my 2x24GB GPUs simulate the Keller’s fork, but have not changed any of the core training / modeling logic. Specifically: I have upstreamed some QOL improvements and basic tweaks to the training script from Nano GPT trainer with some minor modifications / simplifications (such as no dropout). script that Keller Jordan used for his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline his initial baseline . This trainer is very similar to the beginning - with with Andrej Karpathy’s Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer Py Torch GPT-2 trainer from llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c llm.c . This is the Part of the goal of this project is for me to learn as I go, so I am going to start at the 1. Initial setup and baseline ----------------Page (2) Break---------------- tokens/second increased, likely due to the larger batch size (more gradient accumulation run was more data-efficient than the baseline, requiring only 5.07B tokens. However, the After implementing these changes (commit b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f b7bb93f ), the new run time is 7.51 hours . This training time down as fast as possible in the beginning. repository / 8x H100 speedrun. Its not efficient to reinvent the wheel, and I want to get Once again, many of these changes are downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed downstreamed from the modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt modded-nanogpt In addition, learning rate and batch size have been tuned. the performance of cosine schedules. are often easier to reason about / tune around, and they have been show to match of training steps changes the entire schedule. Trapezoidal learning rate schedules de-facto standard, they can be difficult to work with since changing the number 4. Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule Trapezoidal learning rate schedule . While cosine learning rate schedules are the This also eliminates a hyperparameter that needs to be tuned. down training. Since we are speed-running, we will remove gradient clipping. 3. No gradient clipping. Gradient clipping can help stabilize training but it also slows effective in decreasing training time required to reach a certain validation loss. proposed since GPT-2. Re LU^2 is a simple one that has been shown to be 2. Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation Re LU^2 Activation 1 . Many activations that are better than Ge LU have been Ro PE out there so I won’t go into detail here. 1. Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) Ro PE (Rotary Positional Embeddings) . There are many many many many many many many many many many many many many good good good good good good good good good good good good good explanations of 2 paper. The changes are: transformer decoder architecture that have been generally adopted since the original GPT- model that will speed up training. These changes are general improvements to the There are some basic architectural changes and modernizations that can be made to the 2.1 Architectural changes and training tweaks 4. Logit Softcapping 3. Dataloading tweaks 2. Muon optimizer 1. Architectural changes and training tweaks I’ll start with the most impactful/easiest changes first: ----------------Page (3) Break---------------- token embeddings will still be optimized with Adam W. optimize all of the hidden layers of our GPT-2 model. The output | lm_head | layer and the Adam W (e.g. it isn’t meant to optimize Embedding layers). However it can be used to Muon is designed to work on Linear layers, so it is not quite a drop-in replacement for this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post this excellent post by Jeremy Bernstein. gether here here here here here here here here here here here here here . For those interested in a more step-by-step walkthrough of Muon, check out checking out the optimizer comparison for GPT-2 speedrunning that Keller Jordan put to I highly recommend reading the original Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post Muon blog post for more details, as well as connections connections connections connections connections connections connections connections connections connections connections connections connections to approximate second-order optimizers 2 like Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo Shampoo . the gradient updates to approximately orthogonalize each update matrix. Muon has | some | some | some | some | some | some | some | some | some | some | some | some | some by Jordan et al. It is a variant of SGD with Momentum that applies a postprocessing step to The Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer Muon Optimizer is a new optimizer developed with and for the Nano GPT speedrun 2.2 Muon Optimizer effectively and see if I can remove gradient accumulation. the inclusion of Ro PE. Once I have a shorter run time, I will be able to tune more steps which tends to translate to lower throughput) and the architectural changes, such as ----------------Page (4) Break---------------- then step the dataloader for each gradient accumulation step. tweak to our logic to load only the next micro-batch at each step of the dataloader, and we are doing 8 accumulation steps per gradient update). We can instead make a minor batch into smaller chunks (micro-batches) for each gradient accumulation step (recall that Up until now, we have loaded a full-batch of data on each device and then split that full and gradient accumulation logic. significant improvement to our run time. An obvious place to start is with our dataloading That is a ~15% drop in throughput. Recovering most of that throughput could provide a our training throughput has dropped from 221k tokens/second to 187k tokens/second. As we have improved our data efficiency via architecture tweaks and an optimizer change, 2.3 Dataloading Tweaks throughput by switching optimizers. also very similar to the previous run, which is a good sign that we are not losing Muon. The new run time is 4.53 hours , requiring only 3.04B tokens. The tokens/second is Just like on the 8x H100 leaderboard, we observe a massive speedup when switching to ----------------Page (5) Break---------------- c a p s o f t c a p( x , c a p) = c a p ⋅ t a nh ( ) x Soft-capping is essentially a smooth and differentiable version of clipping ⊕ : the Nano GPT speedrun by @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 @Grad62304977 . Logit soft-capping is a technique popularized by Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 Gemma 2 and initially used to improve 2.4 Logit Soft-capping At this point, we code that can train GPT-2 almost twice as fast as the baseline. 4.26 hours , and the changes can be found at d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d d59944d . total number of training steps, so now 3.31B tokens are consumed. The new run time is more consistently hit the 3.28 validation loss target 3 , we have also slightly increased the These tweak brings our throughput back up to 216k tokens/second. In order to make runs use of | torch._inductor.config.coordinate_descent_tuning | . accordance with the new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules new official rules designated on 2025/02/01, we have removed the We also increase our torch version from | 2.5 | to | 2.6 | (which was recently released), and, in ----------------Page (6) Break---------------- tokens | across documents | since we’re just using a simple causal mask. document and cutting that document off before it reaches its end. We are also attending to document starts or stops. That means much of the time we are starting in the middle of a load the next 1024 tokens into an element of the batch without regard for where the been particularly clever about how those sequences are processed. At each step, we simply So far, we’ve been training and evaluating on sequences of 1024 tokens. We also haven’t 3 Longer Training and Evaluation Sequence Length Throughput remained steady at ~218k tokens/second. tuning), the new run time is 4.01 hours , requiring 3.15B tokens (commit 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 12eab44 ). After implementing logit soft-capping with a cap of 30 (and doing some learning-rate that this is helpful. imposing an inductive bias - and since we’re in a relatively small model/low data regime fixed range, which seems to help improve training dynamics. One could argue that this is Logit soft-capping prevents logits from growing excessively large by scaling them to a ----------------Page (7) Break---------------- A natural question to ask at this point is: how long are sequences in our dataset, on average? this loss penalty! sequence. We want to avoid needlessly restarting documents/sequences in order to avoid information with which to make informed predictions about the next token in the later positions. This is because at the beginning of the sequence the LLM has much less Notice how the first twenty-five or so positions have a much higher average loss than the vs sequence position: Cutting off documents in the middle is an especially large issue. See this plot of average loss ----------------Page (8) Break---------------- return q_idx - kv_idx <= window_size def sliding_window_mask(b, h, q_idx, kv_idx): # Limit attention to an N-token window for efficiency return documents[b, q_idx] == documents[b, kv_idx] def document_mask(b, h, q_idx, kv_idx): # Only allow attention within the same document documents = (idx == eot_token).cumsum(dim=1) # Track document boundaries using end-of-text tokens return q_idx >= kv_idx def causal_mask(b, h, q_idx, kv_idx): # Create a causal mask (only attend to past tokens) def make_attn_mask(idx, eot_token, window_size=1024): handles our specific requirements: To implement Flex Attention, we need to define an appropriate attention mask that it ideal for our use case. primary strengths is its ability to efficiently handle sparse, custom attention masks, making benefits of Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention Flash Attention while enabling these improvements. One of Flex Attention’s Fortunately, Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention Flex Attention provides an elegant solution that maintains the performance simultaneously leveraging the computational efficiency of sparse attention patterns. sophisticated attention masking that prevents cross-document attention while of 1 that contains multiple concatenated documents). Second, we’ll implement dimension entirely and instead maximize sequence length (effectively using a “batch size” Taking this approach to its logical conclusion, we’ll eliminate the traditional batch extend our sequence length to minimize document splitting across sequence boundaries. To address the issues identified above, we’ll implement two key improvements. First, we’ll accommodate virtually all documents in our dataset without truncation. sequence length. By increasing the sequence length to >=8192 tokens, we can The data reveals that approximately 20% of documents exceed our current 1024 token ----------------Page (9) Break---------------- Braden Koszarsky , and @Grad62304977 (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) 2024 , Keller Jordan , Jeremy Bernstein , Brendan Rappazzo , @fernbear.bsky.social , Boza Vlado , You Jiacheng , Franz Cesista , References to ~205k tokens/second. See commit d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 d982ed5 for the full details. only 1.88B tokens (a huge data-efficiency improvement). Our throughput dropped slightly 32768 tokens, we observe a massive speedup 5 . The new run time is 2.55 hours , requiring After incorporating Flex Attention with these masks, and increasing our sequence length to limits computational overhead for long sequences. efficient attention pattern that respects document boundaries, maintains causality, and When combined with the and_masks function, these three masks 4 work together to create an below: In order to build intuition about the individual component masks, we visualize them resources. dependencies, while larger windows capture more context at the expense of with a clear tradeoff: smaller windows are more efficient but may miss long-range the current position. This approach balances efficiency with context retention 3. | Sliding Window Mask | : This limits attention to a fixed window of tokens before context within a single document. attending across different documents, which helps the model maintain coherent tracking document boundaries using end-of-text tokens, we prevent tokens from 2. | Document Mask | : This restricts attention to tokens within the same document. By leakage from future tokens. can only attend to previous tokens in the sequence, preventing information 1. | Causal Mask | : Standard in autoregressive language modeling. Ensures that tokens Let’s break down each mask: return and_masks(document_mask, causal_mask, sliding_window_mask) ----------------Page (10) Break---------------- Deriving Muon 2025 , Jeremy Bernstein (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Flex Attention: A Programming Model for Generating Optimized Attention Kernels 2024 , Juechu Dong , Boyuan Feng , Driss Guessous , Yanbo Liang , and Horace He (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Gemma 2: Improving Open Language Models at a Practical Size and Alek Andreev (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Clement Farabet , Elena Buchatskaya , Sebastian Borgeaud , Noah Fiedel , Armand Joulin , Kathleen Kenealy , Robert Dadashi , D. Sculley , Jeanine Banks , Anca Dragan , Slav Petrov , Oriol Vinyals , Jeff Dean , Demis Hassabis , Koray Kavukcuoglu , Kirk , Anand Rao , Minh Giang , Ludovic Peran , Tris Warkentin , Eli Collins , Joelle Barral , Zoubin Ghahramani , Raia Hadsell , Wenming Ye , Woohyun Han , Woosuk Kwon , Xiang Xu , Zhe Shen , Zhitao Gong , Zichuan Wei , Victor Cotruta , Phoebe Kocisky , Tulsee Doshi , Vihan Jain , Vikas Yadav , Vilobh Meshram , Vishal Dharmadhikari , Warren Barkley , Wei Wei , Shruti Garg , Shruti Sheth , Sue Ronstrom , Susan Chan , Timothy Jordan , Ting Yu , Tom Eccles , Tom Hennigan , Tomas Samaneh Saadat , Sara Mc Carthy , Sarah Cogan , Sarah Perrin , Sébastien M. R. Arnold , Sebastian Krause , Shengyang Dai , Pradeep Kuppala , Ramona Comanescu , Ramona Merhej , Reena Jana , Reza Ardeshir Rokni , Rishabh Agarwal , Ryan Mullins , Oscar Wahltinez , Pankil Botarda , Parker Barnes , Paul Barham , Paul Michel , Pengchong Jin , Petko Georgiev , Phil Culliton , Park , Mofi Rahman , Mohit Khatwani , Natalie Dao , Nenshad Bardoliwalla , Nesh Devanathan , Neta Dumai , Nilay Chauhan , Matthew Rahtz , Matthew Watson , Meg Risdal , Mehran Kazemi , Michael Moynihan , Ming Zhang , Minsuk Kahng , Minwoo Machel Reid , Manvinder Singh , Mark Iverson , Martin Görner , Mat Velloso , Mateo Wirth , Matt Davidow , Matt Miller , Lena Heuermann , Leticia Lago , Lilly Mc Nealus , Livio Baldini Soares , Logan Kilpatrick , Lucas Dixon , Luciano Martins , Millican , Keelin Mc Donell , Kelvin Nguyen , Kiranbir Sodhia , Kish Greene , Lars Lowe Sjoesund , Lauren Usui , Laurent Sifre , Amersfoort , Josh Gordon , Josh Lipschultz , Josh Newlan , Ju-yeong Ji , Kareem Mohamed , Kartikeya Badola , Kat Black , Katie Jeff Stanway , Jetha Chan , Jin Peng Zhou , Joana Carrasqueira , Joana Iljazi , Jocelyn Becker , Joe Fernandez , Joost van Hashemi , Hanna Klimczak-Pluci ń ska , Harleen Batra , Harsh Dhand , Ivan Nardini , Jacinda Mein , Jack Zhou , James Svensson , Moreira , Evan Senter , Evgenii Eltyshev , Francesco Visin , Gabriel Rasskin , Gary Wei , Glenn Cameron , Gus Martins , Hadi Weinberger , Dimple Vijaykumar , Dominika Rogozi ń ska , Dustin Herbison , Elisa Bandy , Emma Wang , Eric Noland , Erica Charlie Chen , Chintu Kumar , Chris Perry , Chris Welty , Christopher A. Choquette-Choo , Danila Sinopalnikov , David Amy Shen , Andy Brock , Andy Coenen , Anthony Laforge , Antonia Paterson , Ben Bastian , Bilal Piot , Bo Wu , Brandon Royal , Bachem , Alanna Walton , Aliaksei Severyn , Alicia Parrish , Aliya Ahmad , Allen Hutchison , Alvin Abdagic , Amanda Carl , Sertan Girgin , Nikola Momchev , Matt Hoffman , Shantanu Thakoor , Jean-Bastien Grill , Behnam Neyshabur , Olivier Casbon , Sabela Ramos , Ravin Kumar , Charline Le Lan , Sammy Jerome , Anton Tsitsulin , Nino Vieillard , Piotr Stanczyk , Hussenot , Thomas Mesnard , Bobak Shahriari , Alexandre Ramé , Johan Ferret , Peter Liu , Pouya Tafti , Abe Friesen , Michelle 2024 , Gemma Team , Morgane Riviere , Shreya Pathak , Pier Giuseppe Sessa , Cassidy Hardin , Surya Bhupatiraju , Léonard Old Optimizer, New Norm: An Anthology 2024 , Jeremy Bernstein , and Laker Newhouse (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Shampoo: Preconditioned Stochastic Tensor Optimization 2018 , Vineet Gupta , Tomer Koren , and Yoram Singer (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Muon: An optimizer for hidden layers in neural networks (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) 2024 , Keller Jordan , Yuchen Jin , Vlado Boza , Jiacheng You , Franz Cesista , Laker Newhouse , and Jeremy Bernstein Training Compute-Optimal Large Language Models Laurent Sifre (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Driessche , Bogdan Damoc , Aurelia Guy , Simon Osindero , Karen Simonyan , Erich Elsen , Jack W. Rae , Oriol Vinyals , and Casas , Lisa Anne Hendricks , Johannes Welbl , Aidan Clark , Tom Hennigan , Eric Noland , Katie Millican , George van den 2022 , Jordan Hoffmann , Sebastian Borgeaud , Arthur Mensch , Elena Buchatskaya , Trevor Cai , Eliza Rutherford , Diego de Las Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations online) online) online) online) online) online) online) online) online) online) online) online) online) 2024 , Alexander Hägele , Elie Bakouch , Atli Kosson , Loubna Ben Allal , Leandro Von Werra , and Martin Jaggi (view (view (view (view (view (view (view (view (view (view (view (view (view Primer: Searching for Efficient Transformers for Language Modeling 2022 , David R. So , Wojciech Ma ń ke , Hanxiao Liu , Zihang Dai , Noam Shazeer , and Quoc V. Le (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) Ro Former: Enhanced Transformer with Rotary Position Embedding 2023 , Jianlin Su , Yu Lu , Shengfeng Pan , Ahmed Murtadha , Bo Wen , and Yunfeng Liu (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) hlb-gpt 2024 , Fern (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) (view online) modded-nanogpt: Speedrunning the Nano GPT baseline ----------------Page (11) Break---------------- © 2025 Tyler Romero                                                                              
