Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here Keller Jordan maintains a leaderboard here. At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). I have access to 2x RTX 4090 GPUs and I want to see how fast I can train GPT-2 on them by following the same rules as the Nano GPT speedrun. If I see some success, I may try to transfer my methods to an 8x H100 node for comparison with the main leaderboard. I’ll be documenting my progress here and updating this post as I go. Code can be found in this Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repothis Git Hub repo. Progress so far #Description Record time Training Tokens Tokens/Second Date Commit Log 1111111111111Initial baseline8.13 hours6.44B221k2025/01/16b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8b3c32f8hereherehereherehereherehereherehereherehereherehere 2.12.12.12.12.12.12.12.12.12.12.12.12.1Architectural changes7.51 hours5.07B188k2025/01/18b7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fb7bb93fhereherehereherehereherehereherehereherehereherehere 2.22.22.22.22.22.22.22.22.22.22.22.22.2Muon optimizer4.53 hours3.04B187k2025/01/23b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0b91c2c0hereherehereherehereherehereherehereherehereherehere 2.32.32.32.32.32.32.32.32.32.32.32.32.3Dataloading tweaks4.26 hours3.31B216k2025/02/18d59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dd59944dhereherehereherehereherehereherehereherehereherehere 2.42.42.42.42.42.42.42.42.42.42.42.42.4Logit Soft-capping at 304.01 hours3.15B218k2025/02/2312eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab4412eab44hereherehereherehereherehereherehereherehereherehere 3333333333333Longer Sequence Length2.55 hours1.88B205k2025/03/03d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5d982ed5hereherehereherehereherehereherehereherehereherehere ----------------Page (0) Break---------------- 1. Initial setup and baseline Part of the goal of this project is for me to learn as I go, so I am going to start at the



# What TimeCapsule Found : pattern /Tyler|hours|paper|devices|work|ed|eab|k/gi
<START_TABLE> TABLE_ROW>3 | Longer Sequence Length | 2.55 hours | 1.88B | 205k | 2025/03/03 | d982ed5 | here</TABLE_ROW> <TABLE_ROW>2.4 | Logit Soft-capping at 30 | 4.01 hours | 3.15B | 218k | 2025/02/23 | 12eab44 | here</TABLE_ROW> <TABLE_ROW>2.3 | Dataloading tweaks | 4.26 hours | 3.31B | 216k | 2025/02/18 | d59944d | here</TABLE_ROW> <TABLE_ROW>2.2 | Muon optimizer | 4.53 hours | 3.04B | 187k | 2025/01/23 | b91c2c0 | here</TABLE_ROW> <TABLE_ROW>2.1 | Architectural changes | 7.51 hours | 5.07B | 188k | 2025/01/18 | b7bb93f | here</TABLE_ROW> <TABLE_ROW>1 | Initial baseline | 8.13 hours | 6.44B | 221k | 2025/01/16 | b3c32f8 | here</TABLE_ROW> <TABLE_ROW># | Description | Record time | Training Tokens | Tokens/Second | Date | Commit | Log</TABLE_ROW> <END_TABLE> <START_SECTION:Progress so far> Progress so far <END_SECTION> <START_PARAGRAPH> this Git Hub repo . <END_PARAGRAPH> <START_PARAGRAPH> I’ll be documenting my progress here and updating this post as I go. Code can be found in <END_PARAGRAPH> <START_PARAGRAPH> transfer my methods to an 8x H100 node for comparison with the main leaderboard. <END_PARAGRAPH> <START_PARAGRAPH> by following the same rules as the Nano GPT speedrun. If I see some success, I may try to <END_PARAGRAPH> <TABLE_ROW>I have access to | 2x RTX 4090 GPUs | and I want to see how fast I can train GPT-2 on them</TABLE_ROW> <START_PARAGRAPH> At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). <END_PARAGRAPH> <START_PARAGRAPH> Fine Web as fast as possible on an 8x H100 node. Keller Jordan maintains a
